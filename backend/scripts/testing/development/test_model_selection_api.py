#!/usr/bin/env python3
"""
Audio2Sub API åŠ¨æ€æ¨¡å‹é€‰æ‹©æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒæ¨¡å‹çš„ä¸Šä¼ ã€è½¬å½•å’ŒçŠ¶æ€ç›‘æ§åŠŸèƒ½
"""
import requests
import json
import time
import os
from pathlib import Path

class Audio2SubAPITester:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def test_health(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        print("ğŸ” Testing health endpoint...")
        try:
            response = self.session.get(f"{self.base_url}/health")
            print(f"âœ… Health Status: {response.status_code}")
            if response.status_code == 200:
                print(f"   Response: {response.json()}")
            return response.status_code == 200
        except Exception as e:
            print(f"âŒ Health check failed: {e}")
            return False
    
    def get_available_models(self):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        print("\nğŸ“‹ Getting available models...")
        try:
            response = self.session.get(f"{self.base_url}/models/")
            if response.status_code == 200:
                models_data = response.json()
                models = models_data.get('models', [])
                default_model = models_data.get('default_model', 'base')
                
                print("âœ… Available models:")
                print(f"   Default model: {default_model}")
                for model in models:
                    print(f"   â€¢ {model['name']}: {model['use_case']}")
                    print(f"     Size: {model['size']}, Speed: {model['speed']}, Accuracy: {model['accuracy']}")
                return models
            else:
                print(f"âŒ Failed to get models: {response.status_code}")
                print(f"   Response: {response.text}")
                return []
        except Exception as e:
            print(f"âŒ Error getting models: {e}")
            return []
    
    def create_test_audio(self):
        """åˆ›å»ºä¸€ä¸ªæµ‹è¯•éŸ³é¢‘æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        test_content = """
        Hello, this is a test audio file for Audio2Sub API testing.
        We are testing different Whisper models including tiny, base, and small.
        Each model has different speed and accuracy characteristics.
        This test helps validate the dynamic model selection functionality.
        """
        test_file = Path("test_audio_sample.txt")
        test_file.write_text(test_content.strip())
        print(f"ğŸ“ Created test file: {test_file} ({test_file.stat().st_size} bytes)")
        return test_file
    
    def upload_with_model(self, audio_file, model="base", language="auto", 
                         output_format="both", task="transcribe"):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"""
        print(f"\nğŸš€ Testing upload with model: {model}")
        print(f"   Language: {language}")
        print(f"   Output format: {output_format}")
        print(f"   Task: {task}")
        
        try:
            # å‡†å¤‡æ–‡ä»¶å’Œæ•°æ®
            with open(audio_file, 'rb') as f:
                files = {'file': (audio_file.name, f, 'audio/mpeg')}
                data = {
                    'model': model,
                    'language': language,
                    'output_format': output_format,
                    'task': task
                }
                
                response = self.session.post(
                    f"{self.base_url}/upload/",
                    files=files,
                    data=data
                )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… Upload successful!")
                print(f"   Task ID: {result['task_id']}")
                print(f"   File ID: {result['file_id']}")
                print(f"   Model used: {result.get('model_used', 'N/A')}")
                print(f"   Estimated time: {result.get('estimated_time', 'N/A')} seconds")
                print(f"   Message: {result.get('message', 'N/A')}")
                return result
            else:
                print(f"âŒ Upload failed: {response.status_code}")
                print(f"   Error: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ Upload error: {e}")
            return None
    
    def check_task_status(self, task_id, max_wait=300):
        """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¹¶ç­‰å¾…å®Œæˆ"""
        print(f"\nâ³ Monitoring task: {task_id}")
        
        start_time = time.time()
        last_status = None
        
        while time.time() - start_time < max_wait:
            try:
                response = self.session.get(f"{self.base_url}/status/{task_id}")
                if response.status_code == 200:
                    status_data = response.json()
                    current_status = status_data.get('state', 'UNKNOWN')
                    
                    if current_status != last_status:
                        print(f"   Status: {current_status}")
                        # å®‰å…¨åœ°æ£€æŸ¥åµŒå¥—å­—å…¸
                        result_data = status_data.get('result')
                        if result_data and isinstance(result_data, dict) and 'status' in result_data:
                            print(f"   Details: {result_data['status']}")
                        last_status = current_status
                    
                    if current_status == 'SUCCESS':
                        result = status_data.get('result', {})
                        print("âœ… Task completed successfully!")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                        if 'timing' in result:
                            timing = result['timing']
                            print(f"   Total time: {timing.get('total_time', 'N/A')}s")
                            print(f"   Transcription time: {timing.get('transcription_time', 'N/A')}s")
                        
                        if 'transcription_params' in result:
                            params = result['transcription_params']
                            print(f"   Model used: {params.get('model', 'N/A')}")
                            print(f"   Language: {params.get('language', 'N/A')}")
                        
                        if 'files' in result:
                            files = result['files']
                            print(f"   Generated files: {len(files)}")
                            for file_info in files:
                                print(f"     - {file_info['filename']} ({file_info['type']})")
                        
                        return result
                        
                    elif current_status == 'FAILURE':
                        print(f"âŒ Task failed: {status_data.get('status', 'Unknown error')}")
                        return status_data
                    elif current_status == 'PROGRESS':
                        progress_info = status_data.get('result', {})
                        if isinstance(progress_info, dict) and 'status' in progress_info:
                            print(f"   Progress: {progress_info['status']}")
                    
                    time.sleep(3)
                else:
                    print(f"âŒ Status check failed: {response.status_code}")
                    time.sleep(5)
                    
            except Exception as e:
                print(f"âŒ Status check error: {e}")
                time.sleep(5)
        
        print("â° Timeout waiting for task completion")
        return None
    
    def run_model_comparison_test(self, audio_file):
        """è¿è¡Œä¸åŒæ¨¡å‹çš„æ¯”è¾ƒæµ‹è¯•"""
        print("\nğŸ”¬ Running model comparison tests...")
        
        # æµ‹è¯•ä¸åŒçš„æ¨¡å‹é…ç½®
        test_configs = [
            {
                "model": "tiny",
                "description": "æœ€å¿«é€Ÿåº¦ï¼Œé€‚åˆå®æ—¶å¤„ç†",
                "language": "auto",
                "output_format": "srt"
            },
            {
                "model": "base", 
                "description": "å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡",
                "language": "zh",
                "output_format": "vtt"
            },
            {
                "model": "small",
                "description": "æ›´å¥½çš„å‡†ç¡®åº¦",
                "language": "auto",
                "output_format": "both"
            }
        ]
        
        results = []
        
        for i, config in enumerate(test_configs, 1):
            print(f"\n--- Test {i}/{len(test_configs)}: {config['model']} model ---")
            print(f"Description: {config['description']}")
            
            # ä¸Šä¼ ä»»åŠ¡
            upload_result = self.upload_with_model(
                audio_file,
                model=config["model"],
                language=config["language"],
                output_format=config["output_format"]
            )
            
            if upload_result:
                # ç­‰å¾…å®Œæˆ
                task_result = self.check_task_status(upload_result['task_id'])
                if task_result:
                    results.append({
                        "model": config["model"],
                        "config": config,
                        "task_id": upload_result['task_id'],
                        "result": task_result
                    })
                    print(f"âœ… Test {i} completed")
                else:
                    print(f"âŒ Test {i} failed or timed out")
            else:
                print(f"âŒ Test {i} upload failed")
            
            print("-" * 50)
        
        return results
    
    def print_test_summary(self, results):
        """æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»"""
        print("\n" + "="*60)
        print("ğŸ“Š MODEL COMPARISON SUMMARY")
        print("="*60)
        
        if not results:
            print("âŒ No successful test results to display")
            return
        
        for result in results:
            model = result["model"]
            task_result = result["result"]
            
            print(f"\nğŸ”¹ Model: {model.upper()}")
            
            if 'status' in task_result:
                print(f"   Status: {task_result['status']}")
            
            if 'timing' in task_result:
                timing = task_result['timing']
                print(f"   Processing Time: {timing.get('total_time', 'N/A')}s")
                print(f"   Transcription Time: {timing.get('transcription_time', 'N/A')}s")
            
            if 'transcription_params' in task_result:
                params = task_result['transcription_params']
                print(f"   Parameters:")
                print(f"     - Language: {params.get('language', 'N/A')}")
                print(f"     - Output format: {params.get('output_format', 'N/A')}")
                print(f"     - Task type: {params.get('task_type', 'N/A')}")
            
            if 'files' in task_result:
                files = task_result['files']
                print(f"   Generated Files: {len(files)} files")
                for file_info in files:
                    print(f"     - {file_info['filename']} ({file_info['type']})")
            
            if 'full_text' in task_result:
                text = task_result['full_text']
                preview = text[:100] + "..." if len(text) > 100 else text
                print(f"   Transcription preview: {preview}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = Audio2SubAPITester()
    
    print("ğŸ¯ Audio2Sub API Model Selection Testing")
    print("=" * 50)
    
    # 1. å¥åº·æ£€æŸ¥
    if not tester.test_health():
        print("âŒ API is not available. Please check if the server is running.")
        print("ğŸ’¡ Startup commands:")
        print("   Backend: uvicorn app.main:app --reload")
        print("   Celery: celery -A celery_app.celery_app worker --loglevel=info")
        return False
    
    # 2. è·å–å¯ç”¨æ¨¡å‹
    models = tester.get_available_models()
    if not models:
        print("âŒ No models available")
        return False
    
    # 3. åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = tester.create_test_audio()
    
    try:
        # 4. è¿è¡Œæ¨¡å‹æ¯”è¾ƒæµ‹è¯•
        results = tester.run_model_comparison_test(test_file)
        
        # 5. æ‰“å°ç»“æœæ±‡æ€»
        tester.print_test_summary(results)
        
        if results:
            print(f"\nâœ… Testing completed! Successfully tested {len(results)} models.")
            return True
        else:
            print("\nâŒ No test results to display")
            return False
            
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if test_file.exists():
            test_file.unlink()
            print(f"\nğŸ§¹ Cleaned up test file: {test_file}")

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)

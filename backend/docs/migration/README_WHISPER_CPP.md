# Audio2Sub Backend - Whisper.cpp Integration

## æ¦‚è¿°

è¿™ä¸ªç‰ˆæœ¬çš„Audio2Sub Backendå·²ç»ä»åŸæ¥çš„transformers/torchå®ç°è¿ç§»åˆ°whisper.cppï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½å’Œè®¾å¤‡å…¼å®¹æ€§ã€‚

## ä¸»è¦æ”¹è¿›

### âœ… å·²å®Œæˆçš„è¿ç§»

1. **ä¾èµ–è¿ç§»** - ä»torch/transformersè¿ç§»åˆ°whisper.cpp
2. **é…ç½®é‡æ„** - æ–°çš„whisper.cppä¸“ç”¨é…ç½®å‚æ•°
3. **æ ¸å¿ƒé€»è¾‘é‡å†™** - tasks.pyä½¿ç”¨whisper.cppè¿›è¡Œè¯­éŸ³è¯†åˆ«
4. **å¤šè®¾å¤‡æ”¯æŒ** - CPUã€GPUã€MPSä¸“ç”¨Dockeré…ç½®
5. **æ¨¡å‹ç®¡ç†** - è‡ªåŠ¨ä¸‹è½½å’Œç®¡ç†whisper.cppæ¨¡å‹

### ğŸ—ï¸ æ–°æ¶æ„

- **WhisperManager**: ç®¡ç†whisper.cppæ¨¡å‹å’Œæ¨ç†
- **å¤šDockerfileæ”¯æŒ**: CPUã€GPUã€MPSä¸“ç”¨å®¹å™¨
- **è‡ªåŠ¨è®¾å¤‡æ£€æµ‹**: æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
- **æ¨¡å‹ä¸‹è½½**: è‡ªåŠ¨ä¸‹è½½æ‰€éœ€çš„whisper.cppæ¨¡å‹

## å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨uvè¿è¡Œï¼ˆæ¨èï¼‰

```bash
# å®‰è£…ä¾èµ–
uv sync

# å¯åŠ¨æœåŠ¡
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. ä½¿ç”¨Dockeréƒ¨ç½²

#### è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
```bash
./deploy_whisper.sh auto
```

#### æŒ‡å®šè®¾å¤‡ç±»å‹
```bash
# CPUä¸“ç”¨
./deploy_whisper.sh cpu

# NVIDIA GPU
./deploy_whisper.sh gpu

# Apple Silicon MPS
./deploy_whisper.sh mps
```

### 3. æ‰‹åŠ¨Dockeréƒ¨ç½²

```bash
# CPUç‰ˆæœ¬
docker-compose -f docker-compose.cpu.yml up --build

# GPUç‰ˆæœ¬ï¼ˆéœ€è¦NVIDIA Dockerï¼‰
docker-compose -f docker-compose.gpu.yml up --build

# MPSç‰ˆæœ¬ï¼ˆApple Siliconï¼‰
docker-compose -f docker-compose.mps.yml up --build
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

```bash
# Whisper.cpp é…ç½®
WHISPER_DEVICE=auto          # auto, cpu, cuda, metal
WHISPER_THREADS=0            # 0 = è‡ªåŠ¨æ£€æµ‹
WHISPER_LANGUAGE=auto        # auto æˆ–æŒ‡å®šè¯­è¨€ä»£ç 
WHISPER_TASK=transcribe      # transcribe æˆ– translate
MODEL_NAME=base              # tiny, base, small, medium, large-v1/v2/v3

# éŸ³é¢‘å¤„ç†å‚æ•°
WHISPER_TEMPERATURE=0.0      # é‡‡æ ·æ¸©åº¦
WHISPER_BEST_OF=5           # å€™é€‰æ•°é‡
WHISPER_BEAM_SIZE=5         # æŸæœç´¢å¤§å°
WHISPER_WORD_TIMESTAMPS=true # è¯çº§æ—¶é—´æˆ³

# å­—å¹•ç”Ÿæˆå‚æ•°
MAX_SUBTITLE_DURATION=4      # æœ€å¤§å­—å¹•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
MAX_WORDS_PER_SUBTITLE=8     # æ¯æ¡å­—å¹•æœ€å¤§è¯æ•°
MAX_CHARS_PER_SUBTITLE=50    # æ¯æ¡å­—å¹•æœ€å¤§å­—ç¬¦æ•°
```

### è®¾å¤‡é…ç½®

| è®¾å¤‡ç±»å‹ | é…ç½® | é€‚ç”¨åœºæ™¯ |
|---------|------|----------|
| CPU | `WHISPER_DEVICE=cpu` | å…¼å®¹æ‰€æœ‰ç³»ç»Ÿï¼Œè¾ƒæ…¢ |
| CUDA | `WHISPER_DEVICE=cuda` | NVIDIA GPUåŠ é€Ÿ |
| Metal | `WHISPER_DEVICE=metal` | Apple SiliconåŠ é€Ÿ |

## APIæ¥å£

### å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8000/ping
```

### ä¸Šä¼ æ–‡ä»¶è¿›è¡Œè½¬å½•
```bash
curl -X POST "http://localhost:8000/upload/" \
  -F "file=@your_audio.mp3"
```

### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
```bash
curl http://localhost:8000/status/{task_id}
```

### ä¸‹è½½ç»“æœæ–‡ä»¶
```bash
curl http://localhost:8000/results/{file_id}/{filename}
```

## Dockeré…ç½®è¯¦è§£

### 1. CPUç‰ˆæœ¬ (Dockerfile.cpu)
- åŸºäº `python:3.11-slim`
- é€‚ç”¨äºæ‰€æœ‰ç³»ç»Ÿ
- æœ€ä½³å…¼å®¹æ€§ï¼Œæ€§èƒ½è¾ƒæ…¢

### 2. GPUç‰ˆæœ¬ (Dockerfile.gpu)
- åŸºäº `nvidia/cuda:12.1-devel-ubuntu22.04`
- éœ€è¦NVIDIA Dockerè¿è¡Œæ—¶
- é«˜æ€§èƒ½CUDAåŠ é€Ÿ

### 3. MPSç‰ˆæœ¬ (Dockerfile.mps)
- åŸºäº `python:3.11-slim`
- é’ˆå¯¹Apple Siliconä¼˜åŒ–
- ä½¿ç”¨Metal Performance Shaders

## æ¨¡å‹æ”¯æŒ

æ”¯æŒçš„whisper.cppæ¨¡å‹ï¼š

| æ¨¡å‹ | å¤§å° | ç”¨é€” |
|------|------|------|
| tiny | ~39MB | å¿«é€Ÿè½¬å½•ï¼Œå‡†ç¡®æ€§è¾ƒä½ |
| base | ~142MB | å¹³è¡¡æ€§èƒ½å’Œå‡†ç¡®æ€§ï¼ˆæ¨èï¼‰ |
| small | ~466MB | æ›´é«˜å‡†ç¡®æ€§ |
| medium | ~1.5GB | é«˜å‡†ç¡®æ€§ |
| large-v1/v2/v3 | ~2.9GB | æœ€é«˜å‡†ç¡®æ€§ |

æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `models/` ç›®å½•ã€‚

## æ€§èƒ½å¯¹æ¯”

| è®¾å¤‡ | ç›¸å¯¹é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|
| CPU | 1x | ä½ | å¼€å‘ã€æµ‹è¯•ã€å°æ–‡ä»¶ |
| CUDA GPU | 5-10x | ä¸­ç­‰ | ç”Ÿäº§ç¯å¢ƒã€å¤§æ–‡ä»¶ |
| Apple Silicon MPS | 3-5x | ä¸­ç­‰ | Macå¼€å‘ã€ä¸­ç­‰æ–‡ä»¶ |

## æ•…éšœæ’é™¤

### 1. whisper.cppæœªæ‰¾åˆ°
```bash
# å®‰è£…whisper.cpp
git clone https://github.com/ggerganov/whisper.cpp.git
cd whisper.cpp
make
```

### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° `models/` ç›®å½•
- æ£€æŸ¥ç£ç›˜ç©ºé—´

### 3. GPUä¸å¯ç”¨
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

### 4. å†…å­˜ä¸è¶³
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆtiny/baseï¼‰
- å¢åŠ ç³»ç»Ÿå†…å­˜
- è°ƒæ•´Dockerå†…å­˜é™åˆ¶

## ç›‘æ§å’Œæ—¥å¿—

### Flowerç›‘æ§ç•Œé¢
```bash
# è®¿é—® http://localhost:5555
# æŸ¥çœ‹Celeryä»»åŠ¡çŠ¶æ€å’Œæ€§èƒ½
```

### æŸ¥çœ‹æ—¥å¿—
```bash
# Dockeræ—¥å¿—
docker-compose -f docker-compose.cpu.yml logs -f

# ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose -f docker-compose.cpu.yml logs -f backend
docker-compose -f docker-compose.cpu.yml logs -f celery-worker
```

## å¼€å‘æŒ‡å—

### æœ¬åœ°å¼€å‘
```bash
# å®‰è£…å¼€å‘ä¾èµ–
uv sync

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
uv run uvicorn app.main:app --reload

# å¯åŠ¨Celery worker
uv run celery -A celery_app worker --loglevel=info
```

### æµ‹è¯•
```bash
# è¿è¡Œæµ‹è¯•
uv run python -m pytest

# æµ‹è¯•API
uv run python test_api.py
```

## è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»ºPull Request

## è®¸å¯è¯

MIT License

## æ›´æ–°æ—¥å¿—

### v0.2.0 - Whisper.cppé›†æˆ
- âœ… è¿ç§»åˆ°whisper.cpp
- âœ… å¤šè®¾å¤‡Dockeræ”¯æŒ
- âœ… è‡ªåŠ¨æ¨¡å‹ç®¡ç†
- âœ… æ€§èƒ½ä¼˜åŒ–
- âœ… éƒ¨ç½²è„šæœ¬
